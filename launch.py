import os
import pathlib
import random
import tarfile
import time
from typing import List

import boto3
import paramiko
from paramiko import SSHException

from aws_config import Params

TAR_NAME = "source.tar"
# generated by the AWS 'sudo shutdown' and detected by the monitor to download logs
SHUTDOWN_MESSAGE = "Shutdown scheduled for"


def _compress_folder() -> str:
    """Compress the CWD to the parent/source.tar, return the repository name"""

    file_path = pathlib.Path.cwd().parent / TAR_NAME
    tar = tarfile.open(str(file_path), "w")

    # TODO better zipping and ignoring chosen files (string in the path? exclude)
    with open(pathlib.Path(__file__).parent / "aws_ignore.txt") as f:
        lines = f.readlines()
    excluded = [line.strip() for line in lines]

    def filter_function(tarinfo):
        for ex in excluded:
            if ex in tarinfo.name:
                return None
        else:
            return tarinfo

    folder_name = pathlib.Path.cwd()

    print(f"Compressing {pathlib.Path.cwd()} to {file_path} ")
    tar.add(
        folder_name,
        recursive=True,
        filter=filter_function,
        arcname=folder_name.parts[-1],
    )
    tar.close()
    return folder_name.stem


def start(config: Params):
    ec2 = boto3.resource("ec2", region_name=config.region)

    # credit for the words_alpha.txt file https://github.com/dwyl/english-words
    name = random.choice(
        [word for word in open(pathlib.Path(__file__).parent / "words_alpha.txt")]
    )[:-1]

    common_tags = [
        {"Key": "Owner", "Value": config.owner},
        {"Key": "Group", "Value": config.group},
        {"Key": "Name", "Value": name},
    ]
    tags = common_tags

    # path => key name
    key_name = config.pem_file.split(".")[-2].split("/")[-1]

    instance = ec2.create_instances(
        ImageId=config.ami_id,
        KeyName=key_name,
        InstanceType=config.instance_type,
        MinCount=1,
        MaxCount=1,
        SecurityGroupIds=[],
        TagSpecifications=[{"ResourceType": "instance", "Tags": tags}],
        # terminate the instance after it decides to shutdown
        InstanceInitiatedShutdownBehavior="terminate",
    )[0]
    master_ip = instance.private_ip_address
    print(f"Instance launched, name: {name}, IP: {master_ip}")
    return instance, name


def set_python_path(repo_name: str) -> str:
    return f"cd /home/ubuntu/{repo_name}/ && export PYTHONPATH=."


def setup_script(parallel: int, repo_name: str) -> str:
    """Make the initial part of the script"""

    prepare_logs = []

    # prepare one log file per experiment sequence
    for exp_id in range(parallel):
        prepare_log = """
touch /home/ubuntu/experiment_%s.log
chown ubuntu /home/ubuntu/experiment_%s.log
echo $(date) - Setup... > /home/ubuntu/experiment_%s.log
        """ % (
            exp_id,
            exp_id,
            exp_id,
        )
        prepare_logs.append(prepare_log)

    prepare = "".join(prepare_logs)

    # general setup
    setup = """

# clean the log files if necessary
touch /home/ubuntu/setup.log
> /home/ubuntu/setup.log
chown ubuntu /home/ubuntu/setup.log

# configure the experiment log files
rm /home/ubuntu/experiment_*
%s

# setup the environment
{
# set -x
echo $(date) '---------------- preparing the setup'
%s
# . /home/ubuntu/.bashrc
source activate pytorch_latest_p37
python3 -m pip install -U pip
pip install -r requirements.txt
# pip install -r requirements-lint.txt
# pip install -r requirements-other.txt
# pip install -r requirements-test.txt
echo $(date) '----------------- setup finished'

} >> /home/ubuntu/setup.log 2>&1

    """ % (
        prepare,
        set_python_path(repo_name),
    )
    return setup


def make_experiment_group_script(command_group: List[str], group_id: int) -> str:
    commands = " &&\n".join(command_group)

    worker_script = """

# run the experiment group %s
{
set -x
echo $(date) '------------------ experiment group %s starting'
echo '%s'

%s
echo $(date) '------------------ experiment group %s finished'

} >> /home/ubuntu/experiment_%s.log 2>&1 &

    """ % (
        group_id,
        group_id,
        commands,
        commands,
        group_id,
        group_id,
    )
    return worker_script


def make_script(commands: List[List[str]], repo_name: str):
    max_parallel = len(commands)

    setup = setup_script(max_parallel, repo_name)

    worker_scripts = [
        make_experiment_group_script(group, id) for id, group in enumerate(commands)
    ]

    shutdown_script = """
wait
{
# wait for the background scripts to finish and shutdown
# set +x
sudo shutdown
} >> /home/ubuntu/experiment_0.log 2>&1 &
"""

    # the final script
    script = (
        """
#!/bin/bash
{"""
        + setup
        + "".join(worker_scripts)
        + shutdown_script
        + """
} &
"""
    )

    return script


def make_parallel_commands(
    commands: str, repeats: int, parallel: int
) -> List[List[str]]:
    """ Build list of lists,
     where the inner list contains a group of commands to be executed in a sequence"""

    command_list = commands.split("|")
    command_list = [comm.strip() for comm in command_list]
    command_list = command_list * repeats

    groups = [[] for _ in range(parallel)]
    current_group = 0
    for comm in command_list:
        groups[current_group].append(comm)
        current_group += 1
        if current_group == parallel:
            current_group = 0
    # trim unused groups
    groups = [group for group in groups if len(group) > 0]
    return groups


def upload_and_run(instance, commands: List[List[str]], config: Params):
    instance_ip = instance.private_ip_address

    # prepare the connection
    key = paramiko.RSAKey.from_private_key_file(config.pem_file)
    client = paramiko.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    print(f"compressing the source code to {TAR_NAME}..")
    compressed = _compress_folder()
    print(f"compressed to: {TAR_NAME}")

    def connect(client, instance_ip, key) -> bool:
        for attempt in range(20):
            try:
                print(f"Connecting to {instance_ip}... attempt: {attempt}")
                client.connect(hostname=instance_ip, username="ubuntu", pkey=key)
                print(f"CONNECTED to {instance_ip}")
                return True
            # TODO improve exception handling?
            # except SSHException or TimeoutError or NoValidConnectionsError:
            except:
                time.sleep(1)

        print(f"ERROR: could not connect to the instance: {instance_ip}")
        return False

    result = connect(client, instance_ip, key)
    if not result:
        return

    # Connect/ssh to an instance
    try:
        # check the connection
        stdin, stdout, stderr = client.exec_command("hostname -I | awk '{print $1}'")
        assert instance_ip in str(
            stdout.read()
        ), "IP invalid"  # returned IP of the instance with some garbage
        print(f"CONNECTED to: {instance_ip}")

        # open ftp client and push the source code
        ftp_client = client.open_sftp()
        print(f"client open, uploading sources...")
        if os.path.exists("storage_local.txt"):
            ftp_client.put("storage_local.txt", "/home/ubuntu/storage_local.txt")
        ftp_client.put(f"../{TAR_NAME}", f"/home/ubuntu/{TAR_NAME}")
        ftp_client.close()
        print(f"..sources uploaded")
        # client.close()
    except SSHException:
        print(f"SFTP failed")

    try:
        # Execute a command(cmd) after connecting/ssh to an instance
        stdin, stdout, stderr = client.exec_command(f"tar -xf {TAR_NAME}")
        print(f"decompressed")

        script = make_script(commands, config.repo_name)
        print(f"uploading script: {script}")
        stdin, stdout, stderr = client.exec_command(script)
        # close the client connection once the job is done
        client.close()
    except SSHException:
        print(f"could not read from the client")
    print(f"Machine setup finished!")


def cleanup_instance(instance, name, config: Params):
    script = (
        """
#!/bin/bash

# cleanup Lightning logs, since it crashes sometimes because of this
rm /home/ubuntu/%s/lightning_logs/*

rm /home/ubuntu/experiment_*.log

touch /home/ubuntu/experiment_0.log
chown ubuntu /home/ubuntu/experiment_0.log
echo 'Before setup..' > /home/ubuntu/experiment_0.log

touch /home/ubuntu/setup.log
chown ubuntu /home/ubuntu/setup.log
echo 'Before setup..' > /home/ubuntu/setup.log

    """
        % config.repo_name
    )
    instance_ip = instance["PrivateIpAddress"]
    print(f"Connecting to {name} with IP: {instance_ip}")

    key = paramiko.RSAKey.from_private_key_file(config.pem_file)
    client = paramiko.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    try:
        client.connect(hostname=instance_ip, username="ubuntu", pkey=key)
        print(f"DONE, connected")
        stdin, stdout, stderr = client.exec_command(script)
    except:
        print(f"WARNING: could not connect to the instance, will not perform cleanup!")
